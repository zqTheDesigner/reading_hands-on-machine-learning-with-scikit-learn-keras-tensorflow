{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "class TrainTestSplitPipeline ():\n",
    "    \"\"\"\n",
    "    Split data into training and testing sets. If stratify is required,\n",
    "    will perform stratifield shuffle split based on the selected column\n",
    "\n",
    "    If the column selected as stratified category, and the bins value is\n",
    "    provided, the columns will be first cut in to categories then perform\n",
    "    the split\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    test_size: float, int, default=0.2\n",
    "        If float, should be between 0.0 and 1.0, represent the percentage\n",
    "        of groups to include the test split (rounded up).\n",
    "        If int, should represent the absolute number of test groups. \n",
    "    \n",
    "    Examples\n",
    "    --------\n",
    "    >>> split = TrainTestSplitPipeline()\n",
    "    >>> split.fit(housing)\n",
    "    >>> split.stratify(column='some_column', bins = [0, 1, 2, np.inf])\n",
    "    >>> split.split()\n",
    "    >>> train_set = split.get_train_set()\n",
    "    >>> test_set = split.get_test_set()\n",
    "    \"\"\"\n",
    "    def __init__(self, test_size:float=0.2, label:str=None):\n",
    "        self.test_size = test_size\n",
    "        self.label = label\n",
    "\n",
    "        # For practice purpose, below parameters set to fixed value\n",
    "        self.random_state = 42\n",
    "        self.n_splits = 1\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, X):\n",
    "        self.data = X\n",
    "\n",
    "    def stratify(self, column=None, bins=None ):\n",
    "        \"\"\"\n",
    "        Set stratify categories when perform the splitting\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        column: string\n",
    "            Must be one of the column in the fitted data.\n",
    "        bins: array\n",
    "            If provided, will try to perform a pd.cut() to the \n",
    "            selected column, and use the cutted value as the\n",
    "            stratify category. Label of 1 based index will be \n",
    "            temporarily assigned to the category.\n",
    "        \"\"\"\n",
    "        if bins is not None:\n",
    "            self.stratify_cat = pd.cut(\n",
    "                self.data[column], \n",
    "                bins=bins, \n",
    "                labels=[i+1 for i in range(len(bins)-1)]\n",
    "                )\n",
    "        else:\n",
    "            self.stratify_cat = self.data[column]\n",
    "        \n",
    "        pass\n",
    "\n",
    "    def split(self):\n",
    "        if self.stratify_cat is not None:\n",
    "            split = StratifiedShuffleSplit(\n",
    "                n_splits=self.n_splits, \n",
    "                test_size=self.test_size, \n",
    "                random_state=self.random_state\n",
    "                )\n",
    "            \n",
    "            for train_idx, text_idx in split.split(self.data, self.stratify_cat):\n",
    "                train_set = self.data.loc[train_idx]\n",
    "                test_set = self.data.loc[text_idx]\n",
    "        else:\n",
    "            train_set, test_set = train_test_split(\n",
    "                self.data, \n",
    "                random_state=self.random_state\n",
    "                )\n",
    "        \n",
    "        if self.label is not None:\n",
    "            self.train_feature = train_set.drop(self.label, axis=1)\n",
    "            self.train_label = train_set[self.label].copy()\n",
    "\n",
    "            self.test_feature = test_set.drop(self.label, axis=1)\n",
    "            self.test_label = test_set[self.label].copy()\n",
    "        else:\n",
    "            self.train_feature = train_set.copy()\n",
    "            self.test_feature = test_set.copy()\n",
    "            self.train_label = None\n",
    "            self.test_label = None\n",
    "    \n",
    "    def get_train_set(self):\n",
    "        return self.train_feature, self.train_label\n",
    "    \n",
    "    def get_test_set(self):\n",
    "        return self.test_feature, self.test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class OneHotEncodingPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns):\n",
    "        self.columns = columns\n",
    "        self.encoders = []\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        for col in self.columns:\n",
    "            encoder = OneHotEncoder()\n",
    "            encoder.fit(X[[col]])\n",
    "            self.encoders.append(encoder)\n",
    "        return self\n",
    "    def transform(self, X):\n",
    "        x = X.copy()\n",
    "        for idx,col in enumerate(self.columns):\n",
    "            encoder = self.encoders[idx]\n",
    "            x_onehot =encoder.fit_transform(X[[col]])\n",
    "            \n",
    "            x[encoder.categories_[0]] = x_onehot.toarray()\n",
    "            # pd.concat([x, onehot_df], axis=0)\n",
    "        \n",
    "        return x.drop(self.columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "class FeatureSelectionPipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, combination_function, drop_columns=None):\n",
    "        self.comb_func = combination_function\n",
    "        self.drop_columns = drop_columns\n",
    "    def fit(self, X, y=None):\n",
    "    \n",
    "        return self\n",
    "    def transform(self, X:pd.DataFrame):\n",
    "        x = X.copy()\n",
    "        x = self.comb_func(x)\n",
    "        \n",
    "        if self.drop_columns is not None:\n",
    "            return x.drop(self.drop_columns, axis=1)\n",
    "        else:\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.base import BaseEstimator, TransformerMixin \n",
    "\n",
    "class ImputePipeline(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, strategy, columns=None):\n",
    "        self.imputer = SimpleImputer(strategy=strategy)\n",
    "        self.columns = columns\n",
    "        pass\n",
    "    def fit(self, X, y=None):\n",
    "        if self.columns is not None:\n",
    "            self.imputer.fit(X[self.columns])\n",
    "            return self\n",
    "        else:\n",
    "            self.imputer.fit(X)\n",
    "            return self\n",
    "    def transform(self, X, y=None):\n",
    "        df = X.copy()\n",
    "        if self.columns is not None:\n",
    "            imputed = self.imputer.transform(X[self.columns])\n",
    "            df[self.columns] = imputed\n",
    "            return df\n",
    "        else:\n",
    "            print(X.head())\n",
    "            df = pd.DataFrame(imputed, columns = X.columns, index=X.index)\n",
    "            return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HOUSING_PATH = os.path.join('datasets', 'housing')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_housing_data(housing_path=HOUSING_PATH):\n",
    "\tcsv_path = os.path.join(housing_path, \"housing.csv\")\n",
    "\treturn pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = load_housing_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = TrainTestSplitPipeline(label='median_house_value')\n",
    "split.fit(housing)\n",
    "split.stratify(column='median_income', bins = [0, 1.5, 3.0, 4.5, 6, np.inf])\n",
    "split.split()\n",
    "\n",
    "train_features, train_labels = split.get_train_set()\n",
    "test_features, test_labels = split.get_test_set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features.corrwith(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "def CombineAttributesFunc(data):\n",
    "    data['rooms_per_household'] = data['total_rooms']/data['households']\n",
    "    data['bedrooms_per_room'] = data['total_bedrooms']/data['total_rooms']\n",
    "    return data\n",
    "\n",
    "drop_columns = ['households','total_bedrooms','population','longitude','housing_median_age']\n",
    "impute_columns = ['bedrooms_per_room']\n",
    "\n",
    "feat_processing_pipeline = Pipeline([\n",
    "    ('encoder', OneHotEncodingPipeline(columns=['ocean_proximity'])),\n",
    "    ('feature_selector', FeatureSelectionPipeline(combination_function=CombineAttributesFunc, drop_columns=drop_columns)),\n",
    "    ('imputer',ImputePipeline(strategy='median', columns=impute_columns)),\n",
    "    ('std_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question:\n",
    "\n",
    "- If a new feature that combines 2 existing features (e.g. bedrooms_per_room) have strong correlation with the label, however, one of the feature ('total_rooms') contains missing value. **Should I impute the missing value first, or should I calculate the new feature first (means both of new feature, and current feature will be NA in the beginning), then impute both of them?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features_prepared = feat_processing_pipeline.fit_transform(train_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_features_prepared = feat_processing_pipeline.transform(test_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# svr = SVR(kernel='linear')\n",
    "\n",
    "# scores = cross_val_score(svr, train_features_prepared, train_labels, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "# forest_reg = RandomForestRegressor()\n",
    "\n",
    "# forest_reg_scores = cross_val_score(forest_reg, train_features_prepared, train_labels, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forest_reg_scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import GridSearchCV\n",
    "# from sklearn.svm import SVR\n",
    "\n",
    "# sv_reg = SVR(kernel='linear')\n",
    "\n",
    "# param_grid = [\n",
    "#     {'kernel':['linear']},\n",
    "#     {'kernel':['rbf'], \"C\":[1.0, 2.0], 'gamma':['scale', 'auto']}\n",
    "# ]\n",
    "\n",
    "# grid_search = GridSearchCV(sv_reg, param_grid, cv=3, scoring='neg_mean_squared_error', return_train_score=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final_model = grid_search.best_estimator_\n",
    "\n",
    "# final_predictions = final_model.predict(test_features_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# final_mse = mean_squared_error(test_labels, final_predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
